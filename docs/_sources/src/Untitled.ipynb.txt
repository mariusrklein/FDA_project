{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fac73c4-bed7-4afc-9029-d3fa1018cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import statistics as st\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import papermill as pm\n",
    "import sys\n",
    "sys.path.append('/home/mklein/FDA_project')\n",
    "from src import const\n",
    "from src import correction as corr\n",
    "\n",
    "\n",
    "class ISC:\n",
    "    \n",
    "    def __init__(self, source_path, config = None, verbose=True):\n",
    "        \n",
    "        self.v = verbose\n",
    "        \n",
    "        if config is None:\n",
    "            self.config = const.CONFIG\n",
    "        elif type(config) is str:\n",
    "            with open(config) as json_file:\n",
    "                self.config = json.load(json_file)\n",
    "        else:\n",
    "            self.config = config\n",
    "            \n",
    "        self.config['runtime'] = {}\n",
    "        self.config['runtime']['spacem_dataset_path'] = source_path\n",
    "        \n",
    "        if self.v: \n",
    "            print(self.config)\n",
    "        \n",
    "        self.check_config()\n",
    "        \n",
    "        self.spacem_metadata = self.get_spacem_metadata()\n",
    "        self.samples_list = self.get_samples() \n",
    "        \n",
    "        self.corrections = self.init_sample_corrections()\n",
    "        \n",
    "        self.combine_wells()\n",
    "        \n",
    "        self.write_outputs()\n",
    "        \n",
    "        # self.trigger_evaluation()\n",
    "        \n",
    "        \n",
    "    def check_config(self):\n",
    "        \n",
    "        if not os.path.isdir(self.config['runtime']['spacem_dataset_path']):\n",
    "            raise FileNotFoundError(\"Supplied directory with SpaceM data was not found.\")\n",
    "            \n",
    "        def check_out_directory(path_label, path_name):\n",
    "            if self.config['output']['write_to_input_folder']:\n",
    "                self.config['runtime'][path_label] = os.path.join(self.config['runtime']['spacem_dataset_path'], path_name)\n",
    "            else:\n",
    "                if self.config['output']['external_output_folder'] is None:\n",
    "                    raise FileNotFoundError(\"Forbidden to write to input folder and no directory supplied. Modify your config!\")\n",
    "\n",
    "                self.config['runtime'][path_label] = os.path.join(self.config['output']['external_output_folder'], path_name)\n",
    "\n",
    "            if os.path.isdir(self.config['runtime'][path_label]):\n",
    "                print('Output directory exists. Contained files may be overwritten.')\n",
    "       #          if os.listdir(self.config['runtime'][path_label]):\n",
    "       #              raise FileExistsError(('The supplied output folder \"%s\" already exists and is not empty. '+\n",
    "       #                                     'Refusing to write to this folder. Modify your config!')%self.config['runtime'][path_label])\n",
    "            else:\n",
    "                try:\n",
    "                    os.makedirs(self.config['runtime'][path_label])\n",
    "                except:\n",
    "                    raise Exception('The output directory \"%s\" could not be created. Modify your config!'%self.config['runtime'][path_label])\n",
    "\n",
    "        check_out_directory(path_label = \"out_folder\", path_name = self.config['output']['results_folder'])\n",
    "        check_out_directory(path_label = \"evaluation_folder\", path_name = self.config['evaluation']['evaluation_folder'])\n",
    "        \n",
    "        \n",
    "    def get_spacem_metadata(self):\n",
    "        \n",
    "        metadata_file = os.path.join(self.config['runtime']['spacem_dataset_path'], \n",
    "                                     self.config['input']['spacem_dataset_metadata_file'])\n",
    "        try:\n",
    "            spacem_metadata = pd.read_csv(metadata_file)\n",
    "            spacem_metadata[const.SAMPLE_COL] = spacem_metadata.agg(\n",
    "                re.sub('{(.*?)}', r'{0[\\1]}', self.config['input']['spacem_dataset_metadata_well_name']).format, \n",
    "                axis=1)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError('No spacem_dataset_metadata_file found at %s. Modify your config!'%(\n",
    "                metadata_file))\n",
    "        except KeyError:\n",
    "            raise KeyError('''The spacem_dataset_metadata_file at %s does not contain columns \n",
    "                to create well name using spacem_dataset_metadata_well_name %s. Modify your config!'''%(\n",
    "                metadata_file, self.config['input']['spacem_dataset_metadata_well_name']))\n",
    "        \n",
    "        return spacem_metadata\n",
    "    \n",
    "    def get_samples(self, path = None):\n",
    "        \n",
    "        if path is None:\n",
    "            path = self.config['runtime']['spacem_dataset_path']\n",
    "            \n",
    "        samples = []\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            if 'analysis' in dirnames:\n",
    "                samples.append(re.sub(path+'/?', '', dirpath))\n",
    "        \n",
    "        print(\"Found %d samples in given folder: %s\"%(len(samples), \", \".join(samples)))\n",
    "        samples_list = [s for s in samples if s in list(self.spacem_metadata[const.SAMPLE_COL])]\n",
    "        samples_excluded = [s for s in samples if s not in list(self.spacem_metadata[const.SAMPLE_COL])]\n",
    "        if len(samples_excluded) > 0:\n",
    "            print(\"Excluded %d samples as they did not exist in %s: %s\"%(\n",
    "                len(samples_excluded), \n",
    "                self.config['input']['spacem_dataset_metadata_file'],\n",
    "                \", \".join(samples_excluded)))\n",
    "        \n",
    "        self.config['runtime']['samples'] = samples_list\n",
    "        \n",
    "        return samples_list\n",
    "    \n",
    "    \n",
    "    def init_sample_corrections(self):\n",
    "\n",
    "        corrections = Parallel(n_jobs=2)(\n",
    "        delayed(SampleCorrection)(sample=sample,\n",
    "                                  config=self.config, \n",
    "                                  n_jobs=16) for sample in tqdm(self.samples_list[:2]))\n",
    "        \n",
    "        return corrections\n",
    "    \n",
    "    \n",
    "    def combine_wells(self):\n",
    "        \n",
    "        adatas = {}\n",
    "        gen_adatas = {}\n",
    "        corr_adatas = {}\n",
    "        \n",
    "        for sample in self.corrections:\n",
    "            adatas[sample.name] = sample.cell_adata\n",
    "            gen_adatas[sample.name] = sample.gen_cell_adata\n",
    "            corr_adatas[sample.name] = sample.corr_cell_adata\n",
    "            \n",
    "        self.adata = self.concat_wells(adatas)\n",
    "        self.gen_adata = self.concat_wells(gen_adatas)\n",
    "        self.corr_adata = self.concat_wells(corr_adatas)\n",
    "            \n",
    "    \n",
    "    def concat_wells(self, adata_dict):\n",
    "    \n",
    "        averaged_cols = ['correction_full_pixel_avg_intensities', 'correction_quantreg_slope', 'correction_quantreg_intersect']\n",
    "        counted_cols = ['correction_using_ion_pool']\n",
    "\n",
    "        adata = ad.concat(adata_dict, label='well', index_unique=\"_\", merge=\"first\", join='inner', fill_value=0)\n",
    "        concatenated_var_df = pd.concat({k: v.var for k, v in adata_dict.items()}).select_dtypes(include=[float, bool])\n",
    "\n",
    "        if 'correction_quantreg_slope' in concatenated_var_df.columns:\n",
    "            mean_var_df = concatenated_var_df.reset_index(names = ['well', 'ion']).groupby('ion')[averaged_cols].mean(numeric_only = True)\n",
    "            mean_var_df.columns = ['mean_'+col for col in mean_var_df.columns]\n",
    "\n",
    "            std_var_df = concatenated_var_df.reset_index(names = ['well', 'ion']).groupby('ion')[averaged_cols].std(numeric_only = True)\n",
    "            std_var_df.columns = ['sd_'+col for col in std_var_df.columns]\n",
    "\n",
    "            count_var_df = concatenated_var_df.reset_index(names = ['well', 'ion']).groupby('ion')[counted_cols].sum(numeric_only = True)\n",
    "            count_var_df.columns = ['sum_'+col for col in count_var_df.columns]\n",
    "\n",
    "            dfs = [adata.var, mean_var_df, std_var_df, count_var_df]\n",
    "            adata.var = functools.reduce(lambda left,right: pd.merge(left, right, how='left', left_index=True, right_index=True), dfs)\n",
    "            adata.var['corrected_only_using_pool'] = adata.var['sum_correction_using_ion_pool'] == len(adata_dict)\n",
    "\n",
    "        for col in adata.var.columns:\n",
    "            if col in ['correction_full_pixel_avg_intensities', 'correction_n_datapoints', 'correction_n_iterations', \n",
    "                       'correction_quantreg_intersect', 'correction_quantreg_slope', 'correction_using_ion_pool']:\n",
    "                del adata.var[col]\n",
    "\n",
    "        #sc.tl.pca(adata)\n",
    "        #sc.external.pp.bbknn(adata, batch_key='well')\n",
    "        return adata\n",
    "    \n",
    "    \n",
    "    def write_outputs(self):\n",
    "\n",
    "        self.file_locations = {k: os.path.join(self.config['runtime'][\"out_folder\"], v) for k, v in self.config['output']['file_names'].items()}\n",
    "        self.config['runtime']['file_locations'] = self.file_locations\n",
    "        \n",
    "        self.adata.write(self.file_locations['adata'])\n",
    "        self.gen_adata.write(self.file_locations['generated_adata'])\n",
    "        self.corr_adata.write(self.file_locations['corrected_adata'])\n",
    "\n",
    "   \n",
    "    def trigger_evaluation(self):\n",
    " #       'evaluation': {\n",
    " #      'evaluation_folder': 'ion_suppression_correction/evaluation',\n",
    " #      'run_qc': True,\n",
    " #      'run_results_evaluation': True,\n",
    " #      'run_features_evaluation': True,\n",
    " #  },\n",
    "        \n",
    "        if (not self.config['evaluation']['run_qc'] \n",
    "                and not self.config['evaluation']['run_results_evaluation'] \n",
    "                and not self.config['evaluation']['run_features_evaluation']): \n",
    "            return\n",
    "        \n",
    "        self.evaluation = CorrectionEvaluation(correction=self)\n",
    "        \n",
    "        \n",
    "        \n",
    "class CorrectionEvaluation:\n",
    "    \n",
    "    def __init__(self, correction):\n",
    "        self.data = correction\n",
    "        self.config = self.data.config\n",
    "        \n",
    "        if self.config['evaluation']['run_qc']:\n",
    "            self.run_qc()\n",
    "            \n",
    "        if self.config['evaluation']['run_feature_evaluation']:\n",
    "            self.run_feature_eval()\n",
    "        \n",
    "        if self.config['evaluation']['run_results_evaluation']:\n",
    "            self.run_performance_eval()\n",
    "        \n",
    "        \n",
    "    def run_qc(self):\n",
    "        pm.execute_notebook(\n",
    "            os.path.join('notebook_templates', 'qc.ipynb'),\n",
    "            os.path.join(self.config['runtime']['evaluation_folder'], 'qc.ipynb'),\n",
    "            parameters={'config': self.config}\n",
    "        )\n",
    "    \n",
    "    def run_feature_eval(self):\n",
    "        pm.execute_notebook(\n",
    "            os.path.join('notebook_templates', 'feature_analysis.ipynb'),\n",
    "            os.path.join(self.config['runtime']['evaluation_folder'], 'feature_analysis.ipynb'),\n",
    "            parameters={'config': self.config}\n",
    "        )\n",
    "    \n",
    "    def run_performance_eval(self):\n",
    "        pm.execute_notebook(\n",
    "            os.path.join('notebook_templates', 'results_analysis.ipynb'),\n",
    "            os.path.join(self.config['runtime']['evaluation_folder'], 'results_analysis.ipynb'),\n",
    "            parameters={'config': self.config}\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "class SampleCorrection:\n",
    "    \n",
    "    def __init__(self, sample, config, n_jobs):\n",
    "        \n",
    "        self.name = sample\n",
    "        self.config = config\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        print(\"init %s\" % sample)\n",
    "        \n",
    "        self.spacem_config = self.get_spacem_config()\n",
    "        self.analysis_prefix = os.path.join(self.config['runtime']['spacem_dataset_path'], \n",
    "                                            self.name)\n",
    "        \n",
    "        self.load_data_files()\n",
    "        \n",
    "        self.correct_suppression()\n",
    "\n",
    "        self.deconvolution()\n",
    "        \n",
    "        if self.config['output']['also_write_sample_results']:\n",
    "            sample_out_folder = os.path.join(self.config['runtime'][\"out_folder\"], \n",
    "                                             self.name,\n",
    "                                             self.config['output']['write_sample_folder_path'])\n",
    "\n",
    "            if not os.path.exists(sample_out_folder):\n",
    "                os.makedirs(sample_out_folder)\n",
    "\n",
    "            self.cell_adata.write(os.path.join(sample_out_folder, self.config['output']['file_names']['adata']))\n",
    "            self.gen_cell_adata.write(os.path.join(sample_out_folder, self.config['output']['file_names']['generated_adata']))\n",
    "            self.corr_cell_adata.write(os.path.join(sample_out_folder, self.config['output']['file_names']['corrected_adata']))\n",
    "\n",
    "            if self.config['output']['save_am_files']:\n",
    "                self.corr_am_adata.write(os.path.join(sample_out_folder, self.config['output']['file_names']['corrected_am_adata']))\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_spacem_config(self):\n",
    "        with open(os.path.join(self.config['runtime']['spacem_dataset_path'], \n",
    "                                self.name,\n",
    "                                self.config['input']['spacem_config_file'])) as json_file:\n",
    "            data = json.load(json_file)\n",
    "        return data\n",
    "    \n",
    "    def load_data_files(self):\n",
    "        self.cell_regions = pd.read_csv(os.path.join(self.analysis_prefix,\n",
    "                                                     self.config['input']['cell_regions_file'])\n",
    "                                       )\n",
    "        self.mark_regions = pd.read_csv(os.path.join(self.analysis_prefix,\n",
    "                                                     self.config['input']['mark_regions_file'])\n",
    "                                       )\n",
    "        self.overlap_regions = pd.read_csv(os.path.join(self.analysis_prefix,\n",
    "                                                        self.config['input']['overlap_regions_file'])\n",
    "                                       )\n",
    "\n",
    "        self.am_adata = sc.read(os.path.join(self.analysis_prefix,\n",
    "                                             self.config['input']['am_adata_file'])\n",
    "                                       )\n",
    "        self.cell_adata = sc.read(os.path.join(self.analysis_prefix,\n",
    "                                               self.config['input']['cell_adata_file'])\n",
    "                                       )\n",
    "    def correct_suppression(self):\n",
    "   #    'full_pixel_avg_intensity_method': 'median', \n",
    "   #    'correction_proportion_threshold': 0.01, # smaller values remove influence of sampling proportion better, but may suffer more from outliers\n",
    "   #    'correction_ratios_normalize': False,\n",
    "   #    'correction_intercept': True,\n",
    "        \n",
    "        corr.add_overlap_matrix_spacem(self.am_adata, self.cell_regions, \n",
    "                                       self.mark_regions, self.overlap_regions)\n",
    "        \n",
    "        if self.config['correction']['full_pixel_avg_intensity_method'] == 'median':\n",
    "            method = st.median\n",
    "        elif self.config['correction']['full_pixel_avg_intensity_method'] == 'mean':\n",
    "            method = st.mean\n",
    "        else:\n",
    "            raise NotImplementedError(('Method \"%s\" is not implemented for full_pixel_avg_intensity_method.' +\n",
    "                'Use \"median\" or \"mean\" instead. Modify your config!')%(\n",
    "                self.config['correction']['full_pixel_avg_intensity_method']))\n",
    "        \n",
    "        corr.add_normalization_factors(adata=self.am_adata, method=st.median)\n",
    "        \n",
    "        self.ref_pool = corr.get_reference_pool(self.am_adata, \n",
    "            normalized=self.config['correction']['correction_ratios_normalize'])\n",
    "        \n",
    "        # perform the actual quantile regression\n",
    "        self.corr_am_adata = corr.correct_quantile_inplace(adata=self.am_adata,\n",
    "            reference_ions = self.ref_pool, \n",
    "            correct_intersect = self.config['correction']['correction_intercept'],\n",
    "            normalized = self.config['correction']['correction_ratios_normalize'],\n",
    "            proportion_threshold=self.config['correction']['correction_proportion_threshold'],\n",
    "            n_jobs=self.n_jobs)\n",
    "        \n",
    "    def deconvolution(self):\n",
    "        \n",
    "        overlap_data = corr.get_overlap_data(self.cell_regions, self.mark_regions, self.overlap_regions)\n",
    "        \n",
    "        if self.config['deconvolution']['use_data_from_spacem_configuration']:\n",
    "            deconv_info = self.spacem_config['single_cell_analysis']\n",
    "        else:\n",
    "            deconv_info = self.config['deconvolution']\n",
    "        \n",
    "        self.corr_cell_adata = corr.deconvolution_spacem(adata=self.corr_am_adata,\n",
    "            overlap_data=overlap_data,\n",
    "            raw_adata=self.cell_adata,\n",
    "            deconvolution_params=deconv_info)\n",
    "        \n",
    "        self.gen_cell_adata = corr.deconvolution_spacem(adata=self.am_adata,\n",
    "            overlap_data=overlap_data,\n",
    "            raw_adata=self.cell_adata,\n",
    "            deconvolution_params=deconv_info)\n",
    "\n",
    "        # hand over TPOs to spatiomolecular matrix for downstream analysis\n",
    "        min_overlap = deconv_info['ablation_marks_min_overlap_ratio']\n",
    "        self.corr_cell_adata.obs['list_TPO'] = self.assign_average_tpo(self.am_adata, overlap_data, min_overlap, method=lambda x: \";\".join(x.astype(str)))\n",
    "        self.gen_cell_adata.obs['list_TPO'] = self.assign_average_tpo(self.am_adata, overlap_data, min_overlap, method=lambda x: \";\".join(x.astype(str)))\n",
    "\n",
    "    def assign_average_tpo(self, am_adata, overlap_data, min_overlap, method=np.mean):\n",
    "        if min_overlap is None:\n",
    "            min_overlap = 0\n",
    "\n",
    "        overlap = overlap_data.overlap_regions\n",
    "        overlap['am_id'] = overlap['am_id'].astype(str)\n",
    "        overlap['cell_id'] = overlap['cell_id'].astype(str)\n",
    "        merged_df = pd.merge(overlap[['am_id', 'cell_id']], am_adata.obs[const.TPO], left_on='am_id', right_index=True)\n",
    "        merged_df = merged_df[merged_df[const.TPO] >= min_overlap]\n",
    "\n",
    "        mean_df = merged_df[['cell_id', 'correction_total_pixel_overlap']].groupby('cell_id', group_keys=False).agg(method)\n",
    "        return mean_df[const.TPO]\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34cc2b80-2f71-4f76-b086-7b9b18dbb33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': {'spacem_config_file': 'config.json', 'spacem_dataset_metadata_file': 'metadata.csv', 'spacem_dataset_metadata_well_name': '{row}{col}', 'am_adata_file': 'analysis/ablation_mark_analysis/spatiomolecular_adata.h5ad', 'cell_adata_file': 'analysis/single_cell_analysis/spatiomolecular_adata.h5ad', 'overlap_regions_file': 'analysis/overlap_analysis2/overlap.regions.csv', 'mark_regions_file': 'analysis/overlap_analysis2/ablation_mark.regions.csv', 'cell_regions_file': 'analysis/overlap_analysis2/cell.regions.csv'}, 'correction': {'spacem_library_location': None, 'full_pixel_avg_intensity_method': 'median', 'correction_proportion_threshold': 0.1, 'correction_intercept': True, 'correction_ratios_normalize': False}, 'deconvolution': {'use_data_from_spacem_configuration': True, 'cell_normalization_method': 'weighted_by_overlap_and_sampling_area', 'ablation_marks_min_overlap_ratio': 0.3}, 'evaluation': {'evaluation_folder': 'ion_suppression_correction/evaluation', 'run_qc': True, 'run_results_evaluation': True, 'run_features_evaluation': True}, 'output': {'save_am_files': True, 'results_folder': 'ion_suppression_correction/output', 'write_to_input_folder': True, 'also_write_sample_results': True, 'write_sample_folder_path': 'ion_suppression_correction', 'external_output_folder': None, 'file_names': {'corrected_am_adata': 'corrected_am_sm_matrix.h5ad', 'adata': 'original_batch_sm_matrix.h5ad', 'generated_adata': 'gen_batch_sm_matrix.h5ad', 'corrected_adata': 'corrected_batch_sm_matrix.h5ad'}}, 'runtime': {'spacem_dataset_path': '/home/mklein/Raw Data/220412_Luisa_ScSeahorse_SpaceM'}}\n",
      "Output directory exists. Contained files may be overwritten.\n",
      "Output directory exists. Contained files may be overwritten.\n",
      "Found 19 samples in given folder: A2, A3, B1, B3, B4, C1, D1, E1, E2, E3, E4, F2, F3, F4, G1, G4, H3, H4, J2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 75.37it/s]\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n",
      "/home/mklein/miniconda3/envs/ion_suppression/lib/python3.10/site-packages/statsmodels/regression/quantile_regression.py:191: IterationLimitWarning: Maximum number of iterations (1000) reached.\n",
      "  warnings.warn(\"Maximum number of iterations (\" + str(max_iter) +\n"
     ]
    }
   ],
   "source": [
    "isc = ISC(source_path='/home/mklein/Raw Data/220412_Luisa_ScSeahorse_SpaceM',\n",
    "          #config='/home/mklein/Raw Data/220412_Luisa_ScSeahorse_SpaceM/correction_config.json'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd86872c-443f-49ef-b383-21114c03d0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b45b9-bac0-4d9d-b3d8-9649188c306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init A2\n",
      "Using 4 pixels to calculate full-pixel avereage intensities.\n",
      "init A3\n",
      "Using 17 pixels to calculate full-pixel avereage intensities.\n"
     ]
    }
   ],
   "source": [
    "isc.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "126cae88-bf36-45ab-9ff9-4ec8459e7860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ae0aafb0134ec59e6e2580b260090d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/11 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "isc.trigger_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“ion_suppression_new”",
   "language": "python",
   "name": "ion_suppression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
